# ミニバッチ学習 効率性検証プロジェクト ドキュメント

## 1\. 要件定義書 (Requirements Definition Document)

### 1.1 背景と目的

ニューラルネットワークの学習において、訓練データ全数（例：MNIST 60,000枚）を用いた損失関数の計算は計算資源への負荷が高く、パラメータ更新の頻度が低下する。これを解決するために「ミニバッチ学習」が採用される。
本プロジェクトでは、PythonおよびNumPyを用いて、**「全データを用いた1回の学習ステップ」と「ミニバッチを用いた1回の学習ステップ」の処理時間を計測・比較**し、ミニバッチ学習による反復速度（イテレーション速度）の優位性を定量的に検証する。

### 1.2 機能要件

1.  **データ読み込み機能**
      * MNISTデータセット（訓練画像60,000枚、テスト画像10,000枚）を読み込むことができること。
      * データは正規化（0.0\~1.0）および平坦化（Flatten）されていること。
      * ラベルはOne-hot表現であること。
2.  **模擬ニューラルネットワーク処理機能**
      * 入力層（784）→ 出力層（10）の単純な行列演算（$Y = X \cdot W + B$）を行うこと（重み$W$とバイアス$B$はランダム初期化）。
      * 出力に対してソフトマックス関数を適用できること。
      * 交差エントロピー誤差（バッチ対応版）を計算できること。
3.  **ベンチマーク計測機能**
      * 以下の2つのパターンについて、処理時間を計測できること。
          * **パターンA（フルバッチ）**: 訓練データ全数（60,000枚）を一度に入力し、損失関数を計算するまでの時間。
          * **パターンB（ミニバッチ）**: 訓練データからランダムに抽出した一部（例: 100枚）を入力し、損失関数を計算するまでの時間。
4.  **結果出力機能**
      * 各パターンの処理時間（秒）をコンソールに表示すること。
      * パターンBがパターンAに対して何倍高速化したかを算出・表示すること。

### 1.3 非機能要件

  * **言語**: Python 3.x
  * **ライブラリ**: NumPy（高速な行列演算のため必須）、dataset.mnist（提供されたコードベースを利用）
  * **実行環境**: 一般的なPC（CPU実行を想定）

-----

## 2\. 詳細設計書 (Detailed Design Document)

### 2.1 システム構成図

本スクリプトは単一のPythonファイル（例: `benchmark_batch.py`）として実装する。

[Image of flow chart describing mini-batch learning process]

### 2.2 モジュール・関数設計

#### 2.2.1 前処理・ユーティリティ

  * `softmax(x)`
      * 概要: 出力層の活性化関数。
      * 入力: 行列 `x` (バッチサイズ, 10)
      * 処理: オーバーフロー対策を行った上でソフトマックス計算を行う。
  * `cross_entropy_error(y, t)`
      * 概要: 損失関数（提示されたテキストの実装を利用）。
      * 入力: 出力 `y`, 教師データ `t`
      * 処理: `batch_size` で正規化した交差エントロピー誤差を返す。

#### 2.2.2 メイン処理フロー (`main` 関数)

**ステップ1: データの準備**

  * `load_mnist` を使用して `(x_train, t_train)` を取得。
  * 形状確認: `x_train` は `(60000, 784)`, `t_train` は `(60000, 10)`。

**ステップ2: ネットワークの初期化**

  * 重み $W$: 形状 `(784, 10)`。`np.random.randn` で初期化。
  * バイアス $b$: 形状 `(10,)`。ゼロまたはランダムで初期化。

**ステップ3: パターンA（フルバッチ）の計測**

1.  計測開始時刻を記録 (`time.time()`)。
2.  **行列演算**: `y = np.dot(x_train, W) + b` （60,000行の巨大行列計算）
3.  **活性化**: `y = softmax(y)`
4.  **損失計算**: `loss = cross_entropy_error(y, t_train)`
5.  計測終了時刻を記録し、差分（処理時間）を算出。

**ステップ4: パターンB（ミニバッチ）の計測**

1.  バッチサイズを `100` に設定。
2.  計測開始時刻を記録。
3.  **データ抽出**: `np.random.choice` を使い、`x_train`, `t_train` から100件を抽出して `x_batch`, `t_batch` を作成。
4.  **行列演算**: `y = np.dot(x_batch, W) + b` （100行の軽量行列計算）
5.  **活性化**: `y = softmax(y)`
6.  **損失計算**: `loss = cross_entropy_error(y, t_batch)`
7.  計測終了時刻を記録し、差分（処理時間）を算出。

**ステップ5: 結果の比較と表示**

  * コンソールに以下の形式で出力する。
    ```text
    --- Benchmark Result ---
    Full Batch (60000 data): X.XXXX sec
    Mini Batch (100 data)  : Y.YYYY sec
    Speedup Factor         : Z.ZZ times faster
    ```

### 2.3 期待される挙動と検証ポイント

  * **計算量**:
      * フルバッチは $(60000 \times 784)$ と $(784 \times 10)$ の行列積を行うため、CPU/メモリ負荷が高い。
      * ミニバッチは $(100 \times 784)$ と $(784 \times 10)$ の行列積であり、計算量は単純計算で1/600になる。
  * **検証の結論**:
      * ミニバッチの方が圧倒的に処理時間が短い（「1回の重み更新」にかかる時間が短い）ことが確認できる。
      * これにより、同じ時間内であれば、ミニバッチの方が「何回もパラメータを更新できる（試行回数を稼げる）」ことが実証される。


